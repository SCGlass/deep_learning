{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c19d029-25f5-4d8e-9514-bf3ad9d30026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sam_glass/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sam_glass/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sam_glass/nltk_data...\n",
      "2023-12-13 10:41:22.625901: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 10:41:23.131258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-13 10:41:23.131305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-13 10:41:23.222337: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-13 10:41:23.435207: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 10:41:23.438068: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 10:41:25.517681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2401a-3b28-4a42-ad99-4623e795c5b3",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5766eb4-ca56-44be-9c47-23a102b3d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../week_4/data/imdb_train_data_small.csv')\n",
    "test_df = pd.read_csv('../week_4/data/imdb_test_data_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6342a5a9-2277-422f-96af-351bad324de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a8840-336c-4b42-b6b8-2c4cc53f229f",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Create your own tokenization algorithm. Remember to handle upper/lower case, comma, punctioation and so on.\n",
    "Each word should hava an integer connected to it. Word as key and integer as value in a dict is one way to do it.\n",
    "\n",
    "Tensorflow have tokenization models, but try to bild it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2cc1d-9051-4db9-8fc5-a939df97a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset):\n",
    "    token_map = {}\n",
    "    reverse_token_map = {}\n",
    "    # Your code\n",
    "\n",
    "    token_map['<UNK>'] = 0\n",
    "    return token_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2566c0-015a-4cdf-b88d-b41376df220d",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ff2b32-2e36-4848-8be9-c7e880d3ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30dd737e-3810-4866-8054-5db39ecddace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32681497-e3e4-44f1-a335-6fce79310f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text)\n",
    "    # Your code\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae56ad-156e-4a0d-be28-d17914bcc475",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77df94d-37e1-4615-a8cf-051296763a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea7ac992-e0c9-4e47-836a-c7a07ee22aa5",
   "metadata": {},
   "source": [
    "lemmatizer.lemmatize('Horse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82697754-3845-4cbd-8f21-9b8624612c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    # Your code\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f0f1fe-e10c-4bc1-8533-b22f7b194f3b",
   "metadata": {},
   "source": [
    "# Word embedding and sentiment analysis model\n",
    "We want to create a model that can say if a movie review is bad or good.\n",
    "\n",
    "- Preprocess the text\n",
    "- Convert text to seqiuence of integers\n",
    "- Create architecture that includes embeddings\n",
    "- Build and train your models\n",
    "- Evaluate preformance\n",
    "\n",
    "Building models from scratch is not something you usually do, but those who would like to dig deeper into the math behind Simple RNN, LSTM and GRU can do it by creating the cells from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67861b1-61e9-4b7f-a525-f27c11093338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(embedded_text):\n",
    "    # All sentences should be of the same lenght, but if a sentence is shorter than the longest, pad it.\n",
    "    return padded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3425e-214e-4f0d-89fa-dd61faacbda3",
   "metadata": {},
   "source": [
    "## RNN with tensorflow modules\n",
    "[Simple RNN cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)\n",
    "\n",
    "[Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac733a4c-b314-4b2e-ae0d-295e3e2671d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01152d72-7062-4d00-b3f4-7d1ae3b30f3e",
   "metadata": {},
   "source": [
    "## RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7e11b-131c-49b7-9a3c-0fcd363a632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.Wxh =\n",
    "        self.Whh =\n",
    "        self.bh =\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        h_next = \n",
    "        return h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177958f-cbe0-466c-b39e-78a87ce4f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model Class\n",
    "class MyRNNModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, sequence_length=100):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.rnn_cell = RNNCell(embedding_dim, hidden_dim)\n",
    "        self.Why = \n",
    "        self.by = \n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = \n",
    "        h = \n",
    "\n",
    "        # Process the input sequence\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.rnn_cell(x_t, h)\n",
    "\n",
    "        y = \n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433047d-199f-4aab-858c-f4e25bba4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ececbef-f1f6-4d36-97ee-42ce80b3ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11f54b-f41a-4f5a-b1f4-5c5390d9bb6a",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "[LSTM Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4902ef9-520c-4975-9aff-f89b623104e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198898ec-771e-4b89-b6dc-12944f0c6d19",
   "metadata": {},
   "source": [
    "## LSTM from scrtch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a92a1-e930-41c5-8625-4b6c32adf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Cell Class\n",
    "class LSTMCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Gates: input, forget, cell, output\n",
    "        self.Wi =\n",
    "        self.Wf =\n",
    "        self.Wc =\n",
    "        self.Wo =\n",
    "        self.bi =\n",
    "        self.bf =\n",
    "        self.bc =\n",
    "        self.bo =\n",
    "\n",
    "    def __call__(self, x, h, c):\n",
    "        combined = tf.concat([x, h], 1)\n",
    "\n",
    "        i = \n",
    "        f = \n",
    "        o = \n",
    "        c_ = \n",
    "\n",
    "        c_new = \n",
    "        h_new =\n",
    "\n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f3545-5a8f-45d7-813e-075ae61dd09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model Class\n",
    "class MyLSTMModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.lstm_cell = LSTMCell(embedding_dim, hidden_dim)\n",
    "        self.Why =\n",
    "        self.by =\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x =\n",
    "        h =\n",
    "        c =\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h, c = self.lstm_cell(x_t, h, c)\n",
    "\n",
    "        y =\n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d248f5-0b90-42f7-99ce-351167d00b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    clipped_gradients = [tf.clip_by_norm(g, clip_norm) for g in gradients]\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d6d6e-48d0-4b40-b402-f81b13b62c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3810e-0ae8-4058-b749-799b254f3e81",
   "metadata": {},
   "source": [
    "## GRU\n",
    "[GRU Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da681b6-0ae3-4459-87f4-c8184009ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a06ce2-4f0e-4fb8-b42a-d307982da693",
   "metadata": {},
   "source": [
    "## GRU from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae6a39-8185-4260-9d6b-e1bb45d2b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Cell Class\n",
    "class GRUCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Update gate parameters\n",
    "        self.Wz =\n",
    "        self.bz =\n",
    "\n",
    "        # Reset gate parameters\n",
    "        self.Wr =\n",
    "        self.br =\n",
    "\n",
    "        # Candidate hidden state parameters\n",
    "        self.Wh =\n",
    "        self.bh =\n",
    "        \n",
    "    def __call__(self, x, h):\n",
    "        combined = tf.concat([x, h], 1)\n",
    "\n",
    "        # Update gate\n",
    "        z =\n",
    "\n",
    "        # Reset gate\n",
    "        r =\n",
    "\n",
    "        # Candidate hidden state\n",
    "        combined_reset =\n",
    "        h_candidate =\n",
    "\n",
    "        # New hidden state\n",
    "        h_new =\n",
    "\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6bb3-7353-4aa4-ab88-bf008934661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model Class\n",
    "class MyGRUModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.gru_cell =\n",
    "        self.Why =\n",
    "        self.by =\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x =\n",
    "        h =\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.gru_cell(x_t, h)\n",
    "\n",
    "        y =\n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653c0e5-4d7c-443a-9bce-566c92fcac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3eae4-cda1-4574-bc05-43869b94fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
